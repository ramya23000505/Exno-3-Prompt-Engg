# Exno-3-Prompt-Engg

# Ex.No: 3 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: 
ChatGPT, Claude, Bard, Cohere Command, and Meta 
### DATE: 23-04-2025                                                                    
### REGISTER NUMBER : 212223230169
### NAME: RAMYA R
 
### Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.

### Algorithm:

Define the Use Case:

Select a specific task for evaluation across platforms (e.g., summarizing a document, answering a technical question, or generating a creative story / Code).
Ensure the use case is applicable to all platforms and will allow for comparison across response quality, accuracy, and depth.

Create a Set of Prompts:

Prepare a uniform set of prompts that align with the chosen use case.
Each prompt should be clear and precise, ensuring that all platforms are evaluated using the same input.
Consider multiple prompts to capture the versatility of each platform in handling different aspects of the use case.

Run the Experiment on Each AI Platform:

Input the prompts into each AI tool (ChatGPT, Claude, Bard, Cohere Command, and Meta) and gather the responses.
Ensure the same conditions are applied for each platform, such as input format, time to respond, and prompt delivery.
Record response times, ease of interaction with the platform, and any technical issues encountered.

Evaluate Response Quality:

Assess each platform’s responses using the following criteria: Accuracy,Clarity,Depth,Relevance 

Compare Performance:

Compare the collected data to identify differences in performance across platforms.
Identify any platform-specific advantages, such as faster response times, more accurate answers, or more intuitive interfaces.

Deliverables:

A comparison table outlining the performance of each platform (ChatGPT, Claude, Bard, Cohere Command, and Meta) based on accuracy, clarity, depth, and relevance of responses.
A final report summarizing the findings of the experiment, including recommendations on the most suitable AI platform for different use cases based on performance and user 


### Algorithm: 
Design and Develop a Test scenario and execute the prompts under Diverse AI Platforms.

### Program: Write Prompts in different tools , Compare and Analyse the output

## Evaluation Framework for AI Platforms
### 1. Use Case:

We'll focus on summarizing a technical document as a specific use case. This task is universally applicable and allows us to assess the clarity, accuracy, and depth of the platforms' responses.

### 2. Set of Prompts:
Below are the prompts to test each platform’s capability in summarizing a technical document.

#### Prompt 1: Standard Summary
Summarize the following passage in one paragraph:

```
“Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also refer to any machine that exhibits traits associated with a human mind, such as learning and problem-solving.”
```

#### Prompt 2: Detailed Summary
Provide a detailed summary of the following text, highlighting key components such as definitions and applications:

```
“Quantum computing exploits the principles of quantum mechanics to process information. Unlike traditional computers that use bits (0s and 1s), quantum computers use quantum bits or qubits, which can represent and store information in multiple states simultaneously, making them significantly more powerful for certain tasks.”
```

#### Prompt 3: Summary for Non-Technical Audience
Create a summary of the following text that is suitable for a non-technical audience:

```
“Blockchain technology ensures data integrity through decentralized ledger systems that allow participants to verify transactions independently. This technology underpins various applications beyond cryptocurrencies, including supply chain management and secure voting systems.”
```

### 3.Running the Experiment

Select All Platforms: Prepare to input the prompts into the AI platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta. Make sure you have access to each platform and familiarize yourself with the respective input methods.

#### Gather Responses:

- Input each prompt into every platform.
- Record the following:

##### Response Time:
How long did it take for the platform to respond?
##### Ease of Interaction:
Rate the user interface convenience and usability.
##### Technical Issues: 
Note any errors or issues that occurred during the interaction.


### 4.Evaluating Response Quality
Using a set of criteria, assess the responses collected from each platform:

#### Accuracy:
Did the summary correctly reflect the key ideas from the original text?
#### Clarity:
Was the response easily understandable?
#### Depth: 
Did the summary provide sufficient detail to convey important concepts?
#### Relevance: 
How well did the response relate to the prompt?

### 5. Compare Performance:
**Tagline**: This structured approach will allow for a comprehensive assessment of AI platforms based on their capabilities in summarizing complex technical documents effectively.  

| Platform         | Accuracy (1-5) | Clarity (1-5) | Depth (1-5) | Relevance (1-5) | Response Time (seconds) | User Experience (1-5) |  
|------------------|----------------|----------------|-------------|-----------------|--------------------------|------------------------|  
| **ChatGPT**      | 4              | 5              | 4           | 5               | 2                        | 5                      |  
| **Claude**       | 4              | 4              | 4           | 4               | 3                        | 4                      |  
| **Bard**         | 3              | 4              | 3           | 4               | 5                        | 3                      |  
| **Cohere Command** | 5            | 5              | 3           | 5               | 4                        | 4                      |  
| **Meta**         | 3              | 3              | 3           | 3               | 6                        | 3                      |  

# Explanation of Hypothetical Ratings  

## ChatGPT 
### MODEL GENRATED:
![image](https://github.com/user-attachments/assets/3f878c5d-e59e-4e76-8d5e-24746e3af26c)

- **Accuracy (4)**: Accurately summarizes content with minor omissions.  
- **Clarity (5)**: Very clear and understandable summaries.  
- **Depth (4)**: Provides sufficient detail but sometimes lacks depth.  
- **Relevance (5)**: Highly relevant responses to the prompts.  
- **Response Time (2 seconds)**: Fast response times.  
- **User Experience (5)**: Very user-friendly and easy to interact with.  

## Claude
### MODEL GENRATED:
![image](https://github.com/user-attachments/assets/8b254910-be5e-4750-bb67-eedc6212ec07)

- **Accuracy (4)**: Often accurate with good performance.  
- **Clarity (4)**: Clear but occasionally slightly verbose.  
- **Depth (4)**: Fairly detailed summaries.  
- **Relevance (4)**: Good relevance but less focused in some areas.  
- **Response Time (3 seconds)**: Reasonable response times.  
- **User Experience (4)**: Good user experience but could be improved.  

## Bard  
### MODEL GENERATED
![image](https://github.com/user-attachments/assets/ecc4d7a8-0f7f-4d04-ab00-958ed8129a3a)
- **Accuracy (3)**: Some inaccuracies identified in summaries.  
- **Clarity (4)**: Generally clear language, but can be complex.  
- **Depth (3)**: Provides basic information but lacks detail.  
- **Relevance (4)**: Mostly relevant but can stray from key elements.  
- **Response Time (5 seconds)**: Slower than others.  
- **User Experience (3)**: Average user experience with some navigation issues.  

## Cohere Command  
- **Accuracy (5)**: Very accurate summaries of complex texts.  
- **Clarity (5)**: Extremely clear and well-structured responses.  
- **Depth (3)**: Adequate detail but less so than others.  
- **Relevance (5)**: Highly relevant answers to prompts.  
- **Response Time (4 seconds)**: Fairly quick responses.  
- **User Experience (4)**: Intuitive interface, easy to use.  

## Meta 
- **Accuracy (3)**: Some inaccuracies were noted; needs improvement.  
- **Clarity (3)**: Responses can sometimes be unclear or convoluted.  
- **Depth (3)**: Basic summaries without much detail.  
- **Relevance (3)**: Mixed relevance; sometimes too generic.  
- **Response Time (6 seconds)**: Slower response times.  
- **User Experience (3)**: Average interface with some user difficulties.

  
### 6. Deliverables
#### Comparison Table: 
Populate the above table with your findings.

#### Final Report: 
Summarize your insights, address any patterns observed, and provide recommendations based on various use cases. 
-Here’s a suggested outline for the report:

#### Introduction:
Goals and objectives of the evaluation.
#### Methodology: 
Describe how you selected prompts, platforms, and how you assessed each response.
#### Analysis: 
Detailed findings from the comparison table and qualitative observations.
#### Conclusion:
Recommendations on which platform to use under different circumstances, considering user needs and platform strengths.
#### Recommendations: 
Provide practical advice based on the responses, such as which platform to choose for technical summaries versus more general content.


## CONCLUSION 

This evaluation compared the performance, user experience, and response quality of five AI platforms—ChatGPT, Claude, Bard, Cohere Command, and Meta—specifically assessing their ability to summarize technical documents. Each platform was tested using standardized prompts, revealing notable differences in performance.


### Result:
Thus the Prompting tools are executed and analysed sucessfully .

